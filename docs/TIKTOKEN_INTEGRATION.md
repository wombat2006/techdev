# Tiktoken ãƒˆãƒ¼ã‚¯ãƒ³è¨ˆç®— - çµ±åˆã‚¬ã‚¤ãƒ‰

## ğŸ”¢ Overview

Tiktoken ã¯ OpenAI ã®å…¬å¼ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›ã™ã‚‹ãŸã‚ã®Byte Pair Encoding (BPE) ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚

**å…¬å¼ãƒªãƒã‚¸ãƒˆãƒª**: https://github.com/openai/tiktoken
**ä¸»ãªç‰¹å¾´**: 3-6å€é«˜é€Ÿã€å¯é€†çš„ãƒ»ãƒ­ã‚¹ãƒ¬ã‚¹å¤‰æ›ã€OpenAI ãƒ¢ãƒ‡ãƒ«å¯¾å¿œ

## ğŸ“Š TechSapoã®ç¾çŠ¶

### âœ… ç¾åœ¨ã®å®Ÿè£…
TechSapoã§ã¯ç‹¬è‡ªã® `estimateTokenCount` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ï¼š

```typescript
// ç¾åœ¨ã®å®Ÿè£…ä¾‹ï¼ˆsrc/services/embedding-service.tsï¼‰
private estimateTokenCount(texts: string[]): number {
  return texts.reduce((total, text) => {
    const japaneseCharCount = (text.match(/[\u3040-\u309f\u30a0-\u30ff\u4e00-\u9faf]/g) || []).length;
    const otherCharCount = text.length - japaneseCharCount;
    return total + Math.ceil(japaneseCharCount * 1.5 + otherCharCount * 0.25);
  }, 0);
}
```

### âŒ é™ç•Œãƒ»æ”¹å–„ç‚¹
- **ç²¾åº¦ä¸è¶³**: å®Ÿéš›ã®BPEãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ç•°ãªã‚‹
- **ãƒ¢ãƒ‡ãƒ«éå¯¾å¿œ**: GPT-5å›ºæœ‰ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æœªå¯¾å¿œ
- **ã‚³ã‚¹ãƒˆè¨ˆç®—èª¤å·®**: æ­£ç¢ºãªãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«ã‚ˆã‚‹ã‚³ã‚¹ãƒˆè¨ˆç®—ãŒã§ããªã„

## ğŸ“¦ JavaScript/Node.js ã§ã® Tiktoken å®Ÿè£…

### æ¨å¥¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

#### 1. js-tiktoken (æ¨å¥¨)
```bash
npm install js-tiktoken
```

#### 2. tiktoken-js (ä»£æ›¿)
```bash
npm install @dqbd/tiktoken
```

#### 3. gpt-3-encoder (ãƒ¬ã‚¬ã‚·ãƒ¼)
```bash
npm install gpt-3-encoder
```

## ğŸš€ TechSapoçµ±åˆå®Ÿè£…

### 1. Tiktokenã‚µãƒ¼ãƒ“ã‚¹ä½œæˆ

```typescript
// src/services/tiktoken-service.ts
import { Tiktoken, get_encoding, encoding_for_model } from '@dqbd/tiktoken';

export class TiktokenService {
  private static instance: TiktokenService;
  private encodingCache: Map<string, Tiktoken> = new Map();

  private constructor() {}

  static getInstance(): TiktokenService {
    if (!TiktokenService.instance) {
      TiktokenService.instance = new TiktokenService();
    }
    return TiktokenService.instance;
  }

  /**
   * GPT-5ç”¨ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å–å¾—ï¼ˆTechSapoãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¨™æº–ï¼‰
   */
  getGPT5Encoding(): Tiktoken {
    const cacheKey = 'gpt-5';

    if (!this.encodingCache.has(cacheKey)) {
      try {
        // GPT-5ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆå®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«åã¯ç¢ºèªãŒå¿…è¦ï¼‰
        const encoding = encoding_for_model('gpt-4'); // GPT-5ãŒåˆ©ç”¨å¯èƒ½ã«ãªã‚‹ã¾ã§ã¯GPT-4ã‚’ä½¿ç”¨
        this.encodingCache.set(cacheKey, encoding);
      } catch (error) {
        // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: cl100k_base ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        const encoding = get_encoding('cl100k_base');
        this.encodingCache.set(cacheKey, encoding);
      }
    }

    return this.encodingCache.get(cacheKey)!;
  }

  /**
   * ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
   */
  encode(text: string, model: string = 'gpt-5'): number[] {
    const encoding = this.getModelEncoding(model);
    return encoding.encode(text);
  }

  /**
   * ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«ãƒ‡ã‚³ãƒ¼ãƒ‰
   */
  decode(tokens: number[], model: string = 'gpt-5'): string {
    const encoding = this.getModelEncoding(model);
    return encoding.decode(tokens);
  }

  /**
   * ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
   */
  countTokens(text: string, model: string = 'gpt-5'): number {
    const encoding = this.getModelEncoding(model);
    return encoding.encode(text).length;
  }

  /**
   * è¤‡æ•°ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã¾ã¨ã‚ã¦ã‚«ã‚¦ãƒ³ãƒˆ
   */
  countTokensBatch(texts: string[], model: string = 'gpt-5'): number {
    const encoding = this.getModelEncoding(model);
    return texts.reduce((total, text) => {
      return total + encoding.encode(text).length;
    }, 0);
  }

  /**
   * ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é…åˆ—ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆChat Completionsç”¨ï¼‰
   */
  countTokensForMessages(messages: Array<{role: string, content: string}>, model: string = 'gpt-5'): number {
    const encoding = this.getModelEncoding(model);
    let totalTokens = 0;

    for (const message of messages) {
      // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”¨ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå½¹å‰²ã€åŒºåˆ‡ã‚Šæ–‡å­—ç­‰ï¼‰
      totalTokens += 4; // åŸºæœ¬çš„ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆç”¨

      // å½¹å‰²ï¼ˆroleï¼‰ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°
      totalTokens += encoding.encode(message.role).length;

      // å†…å®¹ï¼ˆcontentï¼‰ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°
      totalTokens += encoding.encode(message.content).length;
    }

    // ä¼šè©±ã®çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³
    totalTokens += 2;

    return totalTokens;
  }

  /**
   * ãƒ¢ãƒ‡ãƒ«åˆ¥ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å–å¾—
   */
  private getModelEncoding(model: string): Tiktoken {
    const cacheKey = model;

    if (!this.encodingCache.has(cacheKey)) {
      try {
        let encoding: Tiktoken;

        switch (model) {
          case 'gpt-5':
            // GPT-5ãŒåˆ©ç”¨å¯èƒ½ã«ãªã‚‹ã¾ã§ã®æš«å®šå¯¾å¿œ
            encoding = get_encoding('cl100k_base');
            break;
          case 'gpt-4':
          case 'gpt-4-turbo':
            encoding = encoding_for_model('gpt-4');
            break;
          case 'gpt-3.5-turbo':
            encoding = encoding_for_model('gpt-3.5-turbo');
            break;
          default:
            // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            encoding = get_encoding('cl100k_base');
        }

        this.encodingCache.set(cacheKey, encoding);
      } catch (error) {
        // ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        const encoding = get_encoding('cl100k_base');
        this.encodingCache.set(cacheKey, encoding);
      }
    }

    return this.encodingCache.get(cacheKey)!;
  }

  /**
   * ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾
   */
  dispose(): void {
    for (const encoding of this.encodingCache.values()) {
      encoding.free();
    }
    this.encodingCache.clear();
  }
}

// ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
export const tiktokenService = TiktokenService.getInstance();
```

### 2. æ—¢å­˜ã‚µãƒ¼ãƒ“ã‚¹ã®æ›´æ–°

#### Wall-Bounce Analyzer ã®æ”¹è‰¯
```typescript
// src/services/wall-bounce-analyzer.ts ã®ä¸€éƒ¨ã‚’æ›´æ–°
import { tiktokenService } from './tiktoken-service';

export class WallBounceAnalyzer {
  // æ—¢å­˜ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ›´æ–°
  private estimateTokens(text: string, model: string = 'gpt-5'): number {
    // å¾“æ¥ã® estimateTokens ã‚’ tiktokenService ã«ç½®ãæ›ãˆ
    return tiktokenService.countTokens(text, model);
  }

  private estimateMessagesTokens(messages: Array<{role: string, content: string}>, model: string = 'gpt-5'): number {
    return tiktokenService.countTokensForMessages(messages, model);
  }

  async executeWallBounce(
    query: string,
    taskType: TaskType,
    options: WallBounceOptions = {}
  ): Promise<WallBounceResult> {
    const startTime = Date.now();

    // æ­£ç¢ºãªãƒˆãƒ¼ã‚¯ãƒ³æ•°è¨ˆç®—ã§ã‚³ã‚¹ãƒˆäºˆæ¸¬
    const inputTokens = tiktokenService.countTokens(query, 'gpt-5');
    const estimatedOutputTokens = Math.min(inputTokens * 2, 4000); // å‡ºåŠ›äºˆæ¸¬

    logger.info('Wall-bounce analysis started', {
      taskType,
      inputTokens,
      estimatedOutputTokens,
      estimatedCost: this.calculateCost('gpt-5', inputTokens, estimatedOutputTokens)
    });

    // æ—¢å­˜ã®Wall-Bounceå‡¦ç†...
    const result = await this.performAnalysis(query, taskType, options);

    // å®Ÿéš›ã®ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’è¨˜éŒ²
    const actualOutputTokens = tiktokenService.countTokens(result.consensus.content, 'gpt-5');

    return {
      ...result,
      token_usage: {
        input_tokens: inputTokens,
        output_tokens: actualOutputTokens,
        total_tokens: inputTokens + actualOutputTokens
      },
      cost_breakdown: {
        input_cost: this.calculateInputCost('gpt-5', inputTokens),
        output_cost: this.calculateOutputCost('gpt-5', actualOutputTokens),
        total_cost: this.calculateCost('gpt-5', inputTokens, actualOutputTokens)
      }
    };
  }

  private calculateCost(model: string, inputTokens: number, outputTokens: number): number {
    // GPT-5ã®æ–™é‡‘ä½“ç³»ï¼ˆå®Ÿéš›ã®ä¾¡æ ¼ã¯ç¢ºèªãŒå¿…è¦ï¼‰
    const pricing = {
      'gpt-5': { input: 0.01, output: 0.03 }, // 1000ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚ŠUSD
      'gpt-4': { input: 0.03, output: 0.06 },
      'gpt-3.5-turbo': { input: 0.001, output: 0.002 }
    };

    const modelPricing = pricing[model as keyof typeof pricing] || pricing['gpt-5'];
    return (inputTokens / 1000) * modelPricing.input + (outputTokens / 1000) * modelPricing.output;
  }
}
```

#### Embedding Service ã®æ”¹è‰¯
```typescript
// src/services/embedding-service.ts ã®ä¸€éƒ¨ã‚’æ›´æ–°
import { tiktokenService } from './tiktoken-service';

export class EmbeddingService {
  // estimateTokenCount ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç½®ãæ›ãˆ
  private estimateTokenCount(texts: string[]): number {
    return tiktokenService.countTokensBatch(texts, 'gpt-5');
  }

  async generateEmbeddings(request: EmbeddingRequest): Promise<EmbeddingResponse> {
    const texts = Array.isArray(request.text) ? request.text : [request.text];

    // æ­£ç¢ºãªãƒˆãƒ¼ã‚¯ãƒ³æ•°è¨ˆç®—
    const accurateTokenCount = this.estimateTokenCount(texts);

    // ã‚³ã‚¹ãƒˆäºˆæ¸¬
    const estimatedCost = this.calculateEmbeddingCost(accurateTokenCount);

    logger.info('Embedding generation started', {
      textCount: texts.length,
      totalTokens: accurateTokenCount,
      estimatedCost,
      model: request.model
    });

    // æ—¢å­˜ã®å‡¦ç†...
    const result = await this.performEmbedding(request);

    return {
      ...result,
      usage: {
        tokenCount: accurateTokenCount,
        processingTime: result.usage.processingTime
      },
      cost: estimatedCost
    };
  }

  private calculateEmbeddingCost(tokens: number): number {
    // Embedding APIã®æ–™é‡‘ï¼ˆ1000ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šï¼‰
    const embeddingPrice = 0.0001; // USD per 1000 tokens
    return (tokens / 1000) * embeddingPrice;
  }
}
```

### 3. ã‚³ã‚¹ãƒˆè¿½è·¡ã‚µãƒ¼ãƒ“ã‚¹

```typescript
// src/services/cost-tracking-enhanced.ts
import { tiktokenService } from './tiktoken-service';
import { getRedisService } from './redis-service';

export interface TokenUsage {
  model: string;
  inputTokens: number;
  outputTokens: number;
  totalTokens: number;
  cost: number;
  timestamp: string;
  sessionId?: string;
  taskType?: string;
}

export class EnhancedCostTrackingService {
  private redis = getRedisService();

  async trackTokenUsage(usage: TokenUsage): Promise<void> {
    const date = new Date().toISOString().split('T')[0];
    const key = `token_usage:${date}`;

    // æ—¥æ¬¡ä½¿ç”¨é‡ã‚’è“„ç©
    await this.redis.hincrbyfloat(key, 'total_cost', usage.cost);
    await this.redis.hincrby(key, 'total_tokens', usage.totalTokens);
    await this.redis.hincrby(key, `${usage.model}_tokens`, usage.totalTokens);
    await this.redis.hincrbyfloat(key, `${usage.model}_cost`, usage.cost);

    // æœ‰åŠ¹æœŸé™ã‚’30æ—¥ã«è¨­å®š
    await this.redis.expire(key, 30 * 24 * 3600);

    // è©³ç´°ãƒ­ã‚°ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ ï¼ˆç›´è¿‘1000ä»¶ã¾ã§ï¼‰
    await this.redis.lpush(`token_usage_log:${date}`, JSON.stringify(usage));
    await this.redis.ltrim(`token_usage_log:${date}`, 0, 999);
  }

  async getDailyUsage(date: string = new Date().toISOString().split('T')[0]): Promise<DailyUsage> {
    const key = `token_usage:${date}`;
    const usage = await this.redis.hgetall(key);

    return {
      date,
      totalCost: parseFloat(usage.total_cost || '0'),
      totalTokens: parseInt(usage.total_tokens || '0'),
      modelBreakdown: {
        'gpt-5': {
          tokens: parseInt(usage['gpt-5_tokens'] || '0'),
          cost: parseFloat(usage['gpt-5_cost'] || '0')
        },
        'gpt-4': {
          tokens: parseInt(usage['gpt-4_tokens'] || '0'),
          cost: parseFloat(usage['gpt-4_cost'] || '0')
        }
      }
    };
  }

  async getMonthlyUsage(year: number = new Date().getFullYear(), month: number = new Date().getMonth() + 1): Promise<MonthlyUsage> {
    const daysInMonth = new Date(year, month, 0).getDate();
    const dailyUsages: DailyUsage[] = [];

    for (let day = 1; day <= daysInMonth; day++) {
      const date = `${year}-${month.toString().padStart(2, '0')}-${day.toString().padStart(2, '0')}`;
      const dailyUsage = await this.getDailyUsage(date);
      dailyUsages.push(dailyUsage);
    }

    const totalCost = dailyUsages.reduce((sum, usage) => sum + usage.totalCost, 0);
    const totalTokens = dailyUsages.reduce((sum, usage) => sum + usage.totalTokens, 0);

    return {
      year,
      month,
      totalCost,
      totalTokens,
      dailyUsages
    };
  }

  async checkBudgetAlert(budgetLimit: number): Promise<BudgetAlert | null> {
    const monthlyUsage = await this.getMonthlyUsage();
    const usagePercent = (monthlyUsage.totalCost / budgetLimit) * 100;

    if (usagePercent >= 90) {
      return {
        level: 'critical',
        message: `äºˆç®—ã®${usagePercent.toFixed(1)}%ã‚’ä½¿ç”¨ã—ã¾ã—ãŸï¼ˆ$${monthlyUsage.totalCost.toFixed(2)}/$${budgetLimit}ï¼‰`,
        currentCost: monthlyUsage.totalCost,
        budgetLimit,
        usagePercent
      };
    } else if (usagePercent >= 75) {
      return {
        level: 'warning',
        message: `äºˆç®—ã®${usagePercent.toFixed(1)}%ã‚’ä½¿ç”¨ã—ã¾ã—ãŸ`,
        currentCost: monthlyUsage.totalCost,
        budgetLimit,
        usagePercent
      };
    }

    return null;
  }
}

export interface DailyUsage {
  date: string;
  totalCost: number;
  totalTokens: number;
  modelBreakdown: Record<string, { tokens: number; cost: number }>;
}

export interface MonthlyUsage {
  year: number;
  month: number;
  totalCost: number;
  totalTokens: number;
  dailyUsages: DailyUsage[];
}

export interface BudgetAlert {
  level: 'warning' | 'critical';
  message: string;
  currentCost: number;
  budgetLimit: number;
  usagePercent: number;
}

export const enhancedCostTrackingService = new EnhancedCostTrackingService();
```

### 4. ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°

```typescript
// src/utils/token-utils.ts
import { tiktokenService } from '../services/tiktoken-service';

/**
 * ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ã«åˆã‚ã›ã¦ãƒˆãƒªãƒŸãƒ³ã‚°
 */
export function trimTextToTokenLimit(text: string, maxTokens: number, model: string = 'gpt-5'): string {
  const tokens = tiktokenService.encode(text, model);

  if (tokens.length <= maxTokens) {
    return text;
  }

  const trimmedTokens = tokens.slice(0, maxTokens);
  return tiktokenService.decode(trimmedTokens, model);
}

/**
 * ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é…åˆ—ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ã«åˆã‚ã›ã¦èª¿æ•´
 */
export function trimMessagesToTokenLimit(
  messages: Array<{role: string, content: string}>,
  maxTokens: number,
  model: string = 'gpt-5'
): Array<{role: string, content: string}> {
  let currentTokens = tiktokenService.countTokensForMessages(messages, model);

  if (currentTokens <= maxTokens) {
    return messages;
  }

  // ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ä¿æŒ
  const systemMessages = messages.filter(msg => msg.role === 'system');
  const otherMessages = messages.filter(msg => msg.role !== 'system');

  // æ–°ã—ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰å„ªå…ˆçš„ã«ä¿æŒ
  const trimmedMessages = [...systemMessages];
  let remainingTokens = maxTokens - tiktokenService.countTokensForMessages(systemMessages, model);

  for (let i = otherMessages.length - 1; i >= 0; i--) {
    const message = otherMessages[i];
    const messageTokens = tiktokenService.countTokensForMessages([message], model);

    if (remainingTokens >= messageTokens) {
      trimmedMessages.push(message);
      remainingTokens -= messageTokens;
    } else {
      break;
    }
  }

  // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®é †åºã‚’å¾©å…ƒ
  return trimmedMessages.sort((a, b) => {
    const aIndex = messages.indexOf(a);
    const bIndex = messages.indexOf(b);
    return aIndex - bIndex;
  });
}

/**
 * ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°è¨ˆç®—
 */
export function calculatePromptTokens(
  template: string,
  variables: Record<string, string>,
  model: string = 'gpt-5'
): { tokens: number; estimatedCost: number; filledPrompt: string } {

  let filledPrompt = template;

  // ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰æ•°ã‚’ç½®æ›
  for (const [key, value] of Object.entries(variables)) {
    filledPrompt = filledPrompt.replace(new RegExp(`\\{${key}\\}`, 'g'), value);
  }

  const tokens = tiktokenService.countTokens(filledPrompt, model);
  const estimatedCost = calculateTokenCost(model, tokens, 0);

  return {
    tokens,
    estimatedCost,
    filledPrompt
  };
}

/**
 * ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒˆãƒ¼ã‚¯ãƒ³ã‚³ã‚¹ãƒˆè¨ˆç®—
 */
export function calculateTokenCost(model: string, inputTokens: number, outputTokens: number): number {
  const pricing = {
    'gpt-5': { input: 0.01, output: 0.03 },
    'gpt-4': { input: 0.03, output: 0.06 },
    'gpt-3.5-turbo': { input: 0.001, output: 0.002 }
  };

  const modelPricing = pricing[model as keyof typeof pricing] || pricing['gpt-5'];
  return (inputTokens / 1000) * modelPricing.input + (outputTokens / 1000) * modelPricing.output;
}
```

## ğŸ”§ å®Ÿè£…æ‰‹é †

### Phase 1: åŸºæœ¬çµ±åˆ (Week 1)
```bash
# 1. Tiktokenãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
npm install @dqbd/tiktoken

# 2. TiktokenServiceã®ä½œæˆ
# src/services/tiktoken-service.ts ã‚’ä½œæˆ

# 3. åŸºæœ¬çš„ãªãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ã®ä½œæˆ
# src/utils/token-utils.ts ã‚’ä½œæˆ
```

### Phase 2: æ—¢å­˜ã‚µãƒ¼ãƒ“ã‚¹æ›´æ–° (Week 2)
```typescript
// 1. Wall-Bounce Analyzer ã®æ›´æ–°
// - estimateTokens ãƒ¡ã‚½ãƒƒãƒ‰ã‚’tiktokenServiceã«ç½®ãæ›ãˆ
// - æ­£ç¢ºãªã‚³ã‚¹ãƒˆè¨ˆç®—ã‚’å®Ÿè£…

// 2. Embedding Service ã®æ›´æ–°
// - estimateTokenCount ãƒ¡ã‚½ãƒƒãƒ‰ã‚’tiktokenServiceã«ç½®ãæ›ãˆ
// - æ­£ç¢ºãªãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«ã‚ˆã‚‹ã‚³ã‚¹ãƒˆè¿½è·¡

// 3. ãã®ä»–ã®ã‚µãƒ¼ãƒ“ã‚¹æ›´æ–°
// - HuggingFaceé–¢é€£ã‚µãƒ¼ãƒ“ã‚¹
// - MCPçµ±åˆã‚µãƒ¼ãƒ“ã‚¹
```

### Phase 3: é«˜åº¦ãªæ©Ÿèƒ½ (Week 3)
```typescript
// 1. EnhancedCostTrackingService ã®å®Ÿè£…
// 2. äºˆç®—ã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½
// 3. ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
// 4. è‡ªå‹•ãƒˆãƒ¼ã‚¯ãƒ³æœ€é©åŒ–
```

## ğŸ“Š ç›£è¦–ãƒ»ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰çµ±åˆ

### Prometheus ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¿½åŠ 
```typescript
// src/metrics/prometheus-client.ts ã«è¿½åŠ 
const tokenMetrics = {
  tokensUsed: new prometheus.Counter({
    name: 'techsapo_tokens_used_total',
    help: 'Total number of tokens used',
    labelNames: ['model', 'service', 'task_type']
  }),

  tokenCost: new prometheus.Counter({
    name: 'techsapo_token_cost_usd_total',
    help: 'Total cost of tokens used in USD',
    labelNames: ['model', 'service']
  }),

  budgetUtilization: new prometheus.Gauge({
    name: 'techsapo_budget_utilization_percent',
    help: 'Current budget utilization percentage'
  })
};

export function recordTokenUsage(model: string, service: string, taskType: string, tokens: number, cost: number) {
  tokenMetrics.tokensUsed.inc({ model, service, task_type: taskType }, tokens);
  tokenMetrics.tokenCost.inc({ model, service }, cost);
}
```

### API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¿½åŠ 
```typescript
// src/routes/token-analytics.ts
import express from 'express';
import { enhancedCostTrackingService } from '../services/cost-tracking-enhanced';

const router = express.Router();

router.get('/usage/daily/:date?', async (req, res) => {
  const date = req.params.date || new Date().toISOString().split('T')[0];
  const usage = await enhancedCostTrackingService.getDailyUsage(date);
  res.json(usage);
});

router.get('/usage/monthly/:year?/:month?', async (req, res) => {
  const year = parseInt(req.params.year || new Date().getFullYear().toString());
  const month = parseInt(req.params.month || (new Date().getMonth() + 1).toString());
  const usage = await enhancedCostTrackingService.getMonthlyUsage(year, month);
  res.json(usage);
});

router.get('/budget/alert', async (req, res) => {
  const budgetLimit = parseFloat(process.env.MONTHLY_BUDGET_LIMIT || '100');
  const alert = await enhancedCostTrackingService.checkBudgetAlert(budgetLimit);
  res.json({ alert });
});

export default router;
```

## âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### 1. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚­ãƒ£ãƒƒã‚·ãƒ¥
```typescript
// TiktokenService ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦é«˜é€ŸåŒ–
private encodingCache: Map<string, Tiktoken> = new Map();
```

### 2. ãƒãƒƒãƒå‡¦ç†
```typescript
// è¤‡æ•°ãƒ†ã‚­ã‚¹ãƒˆã®ä¸€æ‹¬å‡¦ç†ã§åŠ¹ç‡åŒ–
countTokensBatch(texts: string[], model: string = 'gpt-5'): number
```

### 3. éåŒæœŸå‡¦ç†
```typescript
// å¤§é‡ã®ãƒˆãƒ¼ã‚¯ãƒ³è¨ˆç®—ã‚’éåŒæœŸã§å‡¦ç†
async function processLargeTextBatch(texts: string[]): Promise<TokenCount[]>
```

## ğŸ” ãƒ‡ãƒãƒƒã‚°ãƒ»ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰
```typescript
// ç’°å¢ƒå¤‰æ•°ã§ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹åŒ–
const TIKTOKEN_DEBUG = process.env.TIKTOKEN_DEBUG === 'true';

if (TIKTOKEN_DEBUG) {
  console.log('Token breakdown:', {
    text: text.substring(0, 100),
    tokenCount: tokens.length,
    tokens: tokens.slice(0, 10) // æœ€åˆã®10ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¡¨ç¤º
  });
}
```

### ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
```typescript
// Tiktoken ã‚¨ãƒ©ãƒ¼ã®é©åˆ‡ãªå‡¦ç†
try {
  const tokens = tiktokenService.countTokens(text, model);
  return tokens;
} catch (error) {
  logger.warn('Tiktoken encoding failed, falling back to estimation', {
    error: error.message,
    textLength: text.length
  });

  // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®æ¨å®šæ–¹æ³•
  return this.fallbackEstimateTokens(text);
}
```

## ğŸ¯ æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ

### 1. ã‚³ã‚¹ãƒˆç®¡ç†ã®æ”¹å–„
- **æ­£ç¢ºãªäºˆç®—ç®¡ç†**: å®Ÿéš›ã®ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã«åŸºã¥ã
- **ã‚³ã‚¹ãƒˆäºˆæ¸¬ç²¾åº¦**: 95%ä»¥ä¸Šã®ç²¾åº¦ã§ã‚³ã‚¹ãƒˆäºˆæ¸¬
- **äºˆç®—ã‚¢ãƒ©ãƒ¼ãƒˆ**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã®ä½¿ç”¨é‡ç›£è¦–

### 2. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š
- **ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å¯¾å¿œ**: æ­£ç¢ºãªåˆ¶é™å†…ã§ã®ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†
- **APIåŠ¹ç‡åŒ–**: ç„¡é§„ãªãƒˆãƒ¼ã‚¯ãƒ³é€ä¿¡ã®å‰Šæ¸›
- **å¿œç­”æ™‚é–“æœ€é©åŒ–**: é©åˆ‡ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·ã®ç¶­æŒ

### 3. å“è³ªå‘ä¸Š
- **æ­£ç¢ºãªåˆ†æ**: å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«å‹•ä½œã«åŸºã¥ãå‡¦ç†
- **ã‚¨ãƒ©ãƒ¼å‰Šæ¸›**: ãƒˆãƒ¼ã‚¯ãƒ³è¶…éã‚¨ãƒ©ãƒ¼ã®äºˆé˜²
- **ãƒ‡ãƒãƒƒã‚°åŠ¹ç‡**: è©³ç´°ãªãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ã«ã‚ˆã‚‹å•é¡Œç‰¹å®š

ã“ã®Tiktokençµ±åˆã«ã‚ˆã‚Šã€TechSapoã¯æ­£ç¢ºãªãƒˆãƒ¼ã‚¯ãƒ³è¨ˆç®—ã¨ã‚³ã‚¹ãƒˆç®¡ç†ã‚’å®Ÿç¾ã—ã€ã‚ˆã‚ŠåŠ¹ç‡çš„ã§äºˆæ¸¬å¯èƒ½ãªé‹ç”¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚