# OpenAI Node.js SDK - åŸºæœ¬ã‚¬ã‚¤ãƒ‰

## ğŸš€ Overview

OpenAI Node.js SDK (`openai`) ã¯ã€OpenAI APIã¨ã®ã‚„ã‚Šå–ã‚Šã‚’ç°¡å˜ã«ã™ã‚‹ãŸã‚ã®å…¬å¼JavaScriptãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚

**å…¬å¼ãƒªãƒã‚¸ãƒˆãƒª**: https://github.com/openai/openai-node

## ğŸ“¦ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ»ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
npm install openai
```

### åŸºæœ¬è¨­å®š
```typescript
import OpenAI from 'openai';

const client = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY, // ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
  organization: process.env.OPENAI_ORG_ID, // ã‚ªãƒ—ã‚·ãƒ§ãƒ³
});
```

### TechSapoã§ã®è¨­å®šä¾‹
```typescript
// src/config/environment.ts ã§ã®è¨­å®š
export const config = {
  openai: {
    apiKey: process.env.OPENAI_API_KEY!,
    model: 'gpt-5', // TechSapoãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¦ä»¶: GPT-5å°‚ç”¨
    organization: process.env.OPENAI_ORG_ID,
  }
};
```

## ğŸ¯ ä¸»è¦APIæ©Ÿèƒ½

### 1. Responses APIï¼ˆæ–°ã—ã„æ¨™æº–ï¼‰

#### åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•
```typescript
// æ–°ã—ã„Responses API - TechSapoã§æ¨å¥¨
const response = await client.responses.create({
  model: 'gpt-5', // GPT-5å°‚ç”¨ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¦ä»¶ï¼‰
  input: 'ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°ã‚¨ãƒ©ãƒ¼ã®è§£æ±ºæ–¹æ³•ã‚’æ•™ãˆã¦ãã ã•ã„',
  instructions: 'ã‚ãªãŸã¯æ—¥æœ¬èªITæ”¯æ´ã®å°‚é–€å®¶ã§ã™ã€‚',
});

console.log(response.output_text);
```

#### è©³ç´°è¨­å®š
```typescript
const response = await client.responses.create({
  model: 'gpt-5',
  input: userQuery,
  instructions: `
    ã‚ãªãŸã¯æ—¥æœ¬èªITæ”¯æ´ã®å°‚é–€å®¶ã§ã™ã€‚
    - æŠ€è¡“çš„å•é¡Œã‚’æ˜ç¢ºã«åˆ†æ
    - å®Ÿè¡Œå¯èƒ½ãªè§£æ±ºç­–ã‚’ææ¡ˆ
    - ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã®èª¬æ˜
  `,
  store: true, // å¯¾è©±ã‚¹ãƒ†ãƒ¼ãƒˆã®ä¿å­˜
  reasoning: { effort: 'medium' }, // æ¨è«–å“è³ªåˆ¶å¾¡
  metadata: {
    session_id: sessionId,
    task_type: 'it_support'
  }
});
```

### 2. Chat Completions APIï¼ˆå¾“æ¥æ–¹å¼ï¼‰

#### åŸºæœ¬çš„ãªãƒãƒ£ãƒƒãƒˆ
```typescript
const completion = await client.chat.completions.create({
  model: 'gpt-5',
  messages: [
    {
      role: 'system',
      content: 'ã‚ãªãŸã¯æ—¥æœ¬èªITæ”¯æ´ã®å°‚é–€å®¶ã§ã™ã€‚'
    },
    {
      role: 'user',
      content: 'Nginxã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¾ã›ã‚“ã€‚ã©ã†å¯¾å‡¦ã™ã‚Œã°ã‚ˆã„ã§ã™ã‹ï¼Ÿ'
    }
  ],
  temperature: 0.7,
  max_tokens: 1000
});

console.log(completion.choices[0].message.content);
```

#### ãƒ„ãƒ¼ãƒ«çµ±åˆï¼ˆFunction Callingï¼‰
```typescript
const completion = await client.chat.completions.create({
  model: 'gpt-5',
  messages: [
    { role: 'user', content: 'ã‚·ã‚¹ãƒ†ãƒ ã®çŠ¶æ…‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„' }
  ],
  tools: [
    {
      type: 'function',
      function: {
        name: 'checkSystemStatus',
        description: 'ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™',
        parameters: {
          type: 'object',
          properties: {
            service: {
              type: 'string',
              description: 'ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹å'
            },
            detailed: {
              type: 'boolean',
              description: 'è©³ç´°æƒ…å ±ã‚’å–å¾—ã™ã‚‹ã‹'
            }
          },
          required: ['service']
        }
      }
    }
  ],
  tool_choice: 'auto'
});

// ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®å‡¦ç†
const toolCall = completion.choices[0].message.tool_calls?.[0];
if (toolCall) {
  const functionName = toolCall.function.name;
  const args = JSON.parse(toolCall.function.arguments);

  // å®Ÿéš›ã®ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ
  const result = await executeSystemCheck(args.service, args.detailed);

  // çµæœã‚’å«ã‚ã¦å†åº¦APIå‘¼ã³å‡ºã—
  const finalResponse = await client.chat.completions.create({
    model: 'gpt-5',
    messages: [
      ...messages,
      completion.choices[0].message,
      {
        role: 'tool',
        content: JSON.stringify(result),
        tool_call_id: toolCall.id
      }
    ]
  });
}
```

## ğŸŒŠ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ©Ÿèƒ½

### Responses APIã§ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
```typescript
const stream = await client.responses.create({
  model: 'gpt-5',
  input: query,
  stream: true
});

for await (const chunk of stream) {
  if (chunk.type === 'text') {
    process.stdout.write(chunk.content);
  } else if (chunk.type === 'done') {
    console.log('\nå®Œäº†');
    break;
  }
}
```

### Chat Completionsã§ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
```typescript
const stream = await client.chat.completions.create({
  model: 'gpt-5',
  messages: [{ role: 'user', content: 'ITå•é¡Œã®è§£æ±ºæ‰‹é †ã‚’æ•™ãˆã¦ãã ã•ã„' }],
  stream: true
});

for await (const chunk of stream) {
  const content = chunk.choices[0]?.delta?.content;
  if (content) {
    process.stdout.write(content);
  }
}
```

### Express.jsã§ã®Server-Sent Eventsçµ±åˆ
```typescript
// TechSapoã§ã®å®Ÿè£…ä¾‹
app.post('/api/v1/stream-analysis', async (req, res) => {
  const { query } = req.body;

  res.writeHead(200, {
    'Content-Type': 'text/event-stream',
    'Cache-Control': 'no-cache',
    'Connection': 'keep-alive',
    'Access-Control-Allow-Origin': '*'
  });

  try {
    const stream = await client.responses.create({
      model: 'gpt-5',
      input: `ITæ”¯æ´: ${query}`,
      instructions: 'æ®µéšçš„ã«è§£æ±ºç­–ã‚’æç¤ºã—ã¦ãã ã•ã„',
      stream: true
    });

    for await (const chunk of stream) {
      if (chunk.type === 'text') {
        res.write(`data: ${JSON.stringify({
          type: 'content',
          content: chunk.content,
          timestamp: new Date().toISOString()
        })}\n\n`);
      }
    }

    res.write(`data: ${JSON.stringify({
      type: 'done',
      timestamp: new Date().toISOString()
    })}\n\n`);

  } catch (error) {
    res.write(`data: ${JSON.stringify({
      type: 'error',
      error: error.message
    })}\n\n`);
  } finally {
    res.end();
  }
});
```

## ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œ

### ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
```typescript
import fs from 'fs';

// ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
const file = await client.files.create({
  file: fs.createReadStream('path/to/document.pdf'),
  purpose: 'assistants' // ã¾ãŸã¯ 'fine-tune', 'batch'
});

console.log('File uploaded:', file.id);
```

### ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ä½œæˆãƒ»ç®¡ç†
```typescript
// ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ä½œæˆ
const vectorStore = await client.beta.vectorStores.create({
  name: 'TechSapo Knowledge Base',
  expires_after: {
    anchor: 'last_active_at',
    days: 365
  }
});

// ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã«è¿½åŠ 
const vectorStoreFile = await client.beta.vectorStores.files.create(
  vectorStore.id,
  { file_id: file.id }
);

// ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢æ¤œç´¢
const searchResults = await client.beta.vectorStores.fileBatches.create(
  vectorStore.id,
  {
    file_ids: [file.id]
  }
);
```

### TechSapoã§ã®RAGå®Ÿè£…ä¾‹
```typescript
// GoogleDriveé€£æºã§ã®ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
export class OpenAIFileManager {
  private client: OpenAI;

  constructor(apiKey: string) {
    this.client = new OpenAI({ apiKey });
  }

  async uploadDocument(filePath: string, purpose: string = 'assistants'): Promise<string> {
    try {
      const file = await this.client.files.create({
        file: fs.createReadStream(filePath),
        purpose: purpose as any
      });

      logger.info('File uploaded to OpenAI', {
        fileId: file.id,
        filename: file.filename,
        bytes: file.bytes
      });

      return file.id;
    } catch (error) {
      logger.error('Failed to upload file to OpenAI', {
        filePath,
        error: error.message
      });
      throw error;
    }
  }

  async createVectorStore(name: string, fileIds: string[]): Promise<string> {
    const vectorStore = await this.client.beta.vectorStores.create({
      name,
      file_ids: fileIds
    });

    return vectorStore.id;
  }

  async searchInVectorStore(vectorStoreId: string, query: string): Promise<any> {
    const response = await this.client.responses.create({
      model: 'gpt-5',
      input: query,
      tools: [{
        type: 'file_search',
        vector_store_ids: [vectorStoreId]
      }]
    });

    return response;
  }
}
```

## ğŸ› ï¸ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

### åŸºæœ¬çš„ãªã‚¨ãƒ©ãƒ¼å‡¦ç†
```typescript
import OpenAI from 'openai';

try {
  const response = await client.responses.create({
    model: 'gpt-5',
    input: query
  });

  return response.output_text;

} catch (error) {
  if (error instanceof OpenAI.APIError) {
    console.error('OpenAI API Error:', {
      status: error.status,
      message: error.message,
      type: error.type,
      code: error.code
    });

    // ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰åˆ¥å‡¦ç†
    switch (error.status) {
      case 400:
        throw new Error('ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒç„¡åŠ¹ã§ã™: ' + error.message);
      case 401:
        throw new Error('APIã‚­ãƒ¼ãŒç„¡åŠ¹ã§ã™');
      case 429:
        throw new Error('ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„');
      case 500:
        throw new Error('OpenAIã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ');
      default:
        throw new Error(`OpenAI APIã‚¨ãƒ©ãƒ¼ (${error.status}): ${error.message}`);
    }
  } else {
    console.error('Unexpected error:', error);
    throw new Error('äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ');
  }
}
```

### TechSapoã§ã®çµ±åˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
```typescript
// src/utils/openai-error-handler.ts
export class OpenAIErrorHandler {
  static async withRetry<T>(
    operation: () => Promise<T>,
    maxRetries: number = 3,
    baseDelay: number = 1000
  ): Promise<T> {
    let lastError: Error;

    for (let attempt = 1; attempt <= maxRetries; attempt++) {
      try {
        return await operation();
      } catch (error) {
        lastError = error;

        if (error instanceof OpenAI.APIError) {
          // ãƒªãƒˆãƒ©ã‚¤ã—ãªã„æ¡ä»¶
          if ([400, 401, 403].includes(error.status)) {
            throw error;
          }

          // ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®å ´åˆã¯é•·ã‚ã«å¾…ã¤
          if (error.status === 429) {
            const retryAfter = error.headers?.['retry-after'];
            const delay = retryAfter ? parseInt(retryAfter) * 1000 : baseDelay * Math.pow(2, attempt);

            logger.warn(`Rate limited, waiting ${delay}ms before retry ${attempt}/${maxRetries}`);
            await new Promise(resolve => setTimeout(resolve, delay));
            continue;
          }
        }

        if (attempt < maxRetries) {
          const delay = baseDelay * Math.pow(2, attempt - 1);
          logger.warn(`API call failed, retrying in ${delay}ms (${attempt}/${maxRetries})`, {
            error: error.message
          });
          await new Promise(resolve => setTimeout(resolve, delay));
        }
      }
    }

    throw lastError;
  }

  static handleStreamError(error: any, callback: (error: string) => void) {
    if (error instanceof OpenAI.APIError) {
      callback(`OpenAI API Error: ${error.message}`);
    } else {
      callback('ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ');
    }
  }
}
```

## ğŸ”§ é«˜åº¦ãªæ©Ÿèƒ½

### Webhookæ¤œè¨¼
```typescript
import { createHmac } from 'crypto';

export function verifyOpenAIWebhook(
  payload: string,
  signature: string,
  secret: string
): boolean {
  const expectedSignature = createHmac('sha256', secret)
    .update(payload)
    .digest('hex');

  return signature === `sha256=${expectedSignature}`;
}

// Express.jsã§ã®ä½¿ç”¨ä¾‹
app.post('/webhooks/openai', express.raw({ type: 'application/json' }), (req, res) => {
  const signature = req.headers['openai-signature'] as string;
  const payload = req.body.toString();

  if (!verifyOpenAIWebhook(payload, signature, process.env.OPENAI_WEBHOOK_SECRET!)) {
    return res.status(400).send('Invalid signature');
  }

  const event = JSON.parse(payload);
  // Webhookã‚¤ãƒ™ãƒ³ãƒˆã®å‡¦ç†
  console.log('Received webhook event:', event.type);

  res.status(200).send('OK');
});
```

### Batch Processingï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰
```typescript
// å¤§é‡ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒãƒƒãƒã§å‡¦ç†
const batchInput = [
  { custom_id: 'request-1', method: 'POST', url: '/v1/responses', body: {
    model: 'gpt-5',
    input: 'ãƒ­ã‚°ã‚¨ãƒ©ãƒ¼ã®åˆ†æ: Error 500'
  }},
  { custom_id: 'request-2', method: 'POST', url: '/v1/responses', body: {
    model: 'gpt-5',
    input: 'ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šå•é¡Œã®è§£æ±º'
  }}
];

// ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
const batchFile = await client.files.create({
  file: Buffer.from(batchInput.map(req => JSON.stringify(req)).join('\n')),
  purpose: 'batch'
});

// ãƒãƒƒãƒå‡¦ç†é–‹å§‹
const batch = await client.batches.create({
  input_file_id: batchFile.id,
  endpoint: '/v1/responses',
  completion_window: '24h'
});

// ãƒãƒƒãƒçŠ¶æ…‹ç¢ºèª
const batchStatus = await client.batches.retrieve(batch.id);
console.log('Batch status:', batchStatus.status);
```

## ğŸŒ Azure OpenAIå¯¾å¿œ

### Azure OpenAIè¨­å®š
```typescript
import { AzureOpenAI } from 'openai';

const azureClient = new AzureOpenAI({
  apiKey: process.env.AZURE_OPENAI_API_KEY,
  endpoint: process.env.AZURE_OPENAI_ENDPOINT,
  apiVersion: '2024-02-15-preview',
  deployment: 'gpt-5-deployment' // Azureã§ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå
});

// ä½¿ç”¨æ–¹æ³•ã¯é€šå¸¸ã®OpenAIã¨åŒã˜
const response = await azureClient.responses.create({
  input: 'ITå•é¡Œã®è§£æ±ºæ–¹æ³•'
});
```

## ğŸ“Š ä½¿ç”¨é‡ç›£è¦–ãƒ»ã‚³ã‚¹ãƒˆç®¡ç†

### ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡è¿½è·¡
```typescript
export class OpenAIUsageTracker {
  private totalTokens: number = 0;
  private totalCost: number = 0;
  private redis: Redis;

  constructor(redis: Redis) {
    this.redis = redis;
  }

  async trackUsage(model: string, promptTokens: number, completionTokens: number) {
    const totalTokens = promptTokens + completionTokens;
    const cost = this.calculateCost(model, promptTokens, completionTokens);

    this.totalTokens += totalTokens;
    this.totalCost += cost;

    // Redis ã«ä¿å­˜
    const today = new Date().toISOString().split('T')[0];
    await this.redis.hincrby(`usage:${today}`, 'tokens', totalTokens);
    await this.redis.hincrbyfloat(`usage:${today}`, 'cost', cost);

    // äºˆç®—ãƒã‚§ãƒƒã‚¯
    if (this.totalCost > parseFloat(process.env.OPENAI_BUDGET_LIMIT || '100')) {
      logger.warn('OpenAI budget limit exceeded', {
        currentCost: this.totalCost,
        limit: process.env.OPENAI_BUDGET_LIMIT
      });
    }
  }

  private calculateCost(model: string, promptTokens: number, completionTokens: number): number {
    const pricing = {
      'gpt-5': { prompt: 0.01, completion: 0.03 }, // 1000ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®USD
      'gpt-4': { prompt: 0.03, completion: 0.06 },
      'gpt-3.5-turbo': { prompt: 0.001, completion: 0.002 }
    };

    const modelPricing = pricing[model] || pricing['gpt-5'];
    return (promptTokens / 1000) * modelPricing.prompt +
           (completionTokens / 1000) * modelPricing.completion;
  }

  async getDailyUsage(date: string = new Date().toISOString().split('T')[0]) {
    const usage = await this.redis.hgetall(`usage:${date}`);
    return {
      date,
      tokens: parseInt(usage.tokens || '0'),
      cost: parseFloat(usage.cost || '0')
    };
  }
}
```

## ğŸ”„ TechSapoçµ±åˆãƒ‘ã‚¿ãƒ¼ãƒ³

### Wall-Bounce Analyzerã¨ã®çµ±åˆ
```typescript
// æ—¢å­˜ã®Wall-Bounceåˆ†æã«OpenAI SDKã®é«˜åº¦æ©Ÿèƒ½ã‚’çµ±åˆ
export class EnhancedOpenAIIntegration {
  private client: OpenAI;
  private usageTracker: OpenAIUsageTracker;

  async executeEnhancedAnalysis(query: string, options: AnalysisOptions) {
    const startTime = Date.now();

    try {
      // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¿œç­”
      if (options.streaming) {
        return await this.executeStreamingAnalysis(query, options);
      }

      // ãƒãƒƒãƒå‡¦ç†ã§è¤‡æ•°ã‚¯ã‚¨ãƒªã‚’åŠ¹ç‡çš„ã«å‡¦ç†
      if (options.batch) {
        return await this.executeBatchAnalysis(query, options);
      }

      // æ¨™æº–åˆ†æ
      const response = await OpenAIErrorHandler.withRetry(async () => {
        return await this.client.responses.create({
          model: 'gpt-5',
          input: query,
          instructions: options.instructions || 'ITå•é¡Œè§£æ±ºã®å°‚é–€å®¶ã¨ã—ã¦å›ç­”',
          store: options.sessionId ? true : false,
          metadata: {
            session_id: options.sessionId,
            task_type: options.taskType,
            timestamp: new Date().toISOString()
          }
        });
      });

      // ä½¿ç”¨é‡è¿½è·¡
      await this.usageTracker.trackUsage(
        'gpt-5',
        response.usage?.prompt_tokens || 0,
        response.usage?.completion_tokens || 0
      );

      return {
        content: response.output_text,
        metadata: {
          processingTime: Date.now() - startTime,
          tokenUsage: response.usage,
          model: 'gpt-5'
        }
      };

    } catch (error) {
      logger.error('Enhanced OpenAI analysis failed', {
        query: query.substring(0, 100),
        error: error.message,
        processingTime: Date.now() - startTime
      });
      throw error;
    }
  }

  private async executeStreamingAnalysis(query: string, options: AnalysisOptions) {
    const stream = await this.client.responses.create({
      model: 'gpt-5',
      input: query,
      instructions: options.instructions,
      stream: true
    });

    let fullContent = '';
    const chunks: string[] = [];

    for await (const chunk of stream) {
      if (chunk.type === 'text') {
        chunks.push(chunk.content);
        fullContent += chunk.content;

        // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if (options.onChunk) {
          options.onChunk(chunk.content);
        }
      }
    }

    return {
      content: fullContent,
      chunks,
      streaming: true
    };
  }
}
```

## ğŸ¯ ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 1. ç’°å¢ƒè¨­å®šã®æœ€é©åŒ–
```typescript
// æœ¬ç•ªç’°å¢ƒã¨devç’°å¢ƒã§ã®è¨­å®šåˆ†é›¢
const openaiConfig = {
  apiKey: process.env.OPENAI_API_KEY,
  organization: process.env.OPENAI_ORG_ID,
  timeout: process.env.NODE_ENV === 'production' ? 60000 : 30000,
  maxRetries: process.env.NODE_ENV === 'production' ? 3 : 1,
};
```

### 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç®¡ç†
```typescript
export const PromptTemplates = {
  IT_SUPPORT: `
ã‚ãªãŸã¯æ—¥æœ¬èªITæ”¯æ´ã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®å•é¡Œã«ã¤ã„ã¦åˆ†æã—ã€è§£æ±ºç­–ã‚’ææ¡ˆã—ã¦ãã ã•ã„ï¼š

å•é¡Œ: {query}

å›ç­”å½¢å¼ï¼š
1. å•é¡Œã®åˆ†æ
2. æ ¹æœ¬åŸå› 
3. è§£æ±ºæ‰‹é †ï¼ˆã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
4. äºˆé˜²ç­–
`,

  LOG_ANALYSIS: `
ä»¥ä¸‹ã®ãƒ­ã‚°ã‚’åˆ†æã—ã€å•é¡Œã‚’ç‰¹å®šã—ã¦ãã ã•ã„ï¼š

ãƒ­ã‚°: {logContent}
ã‚·ã‚¹ãƒ†ãƒ : {systemContext}

åˆ†æçµæœï¼š
- å•é¡Œç¨®åˆ¥ï¼š
- æ·±åˆ»åº¦ï¼š
- æ¨å¥¨å¯¾å¿œï¼š
`
};
```

### 3. ãƒ¬ã‚¹ãƒãƒ³ã‚¹å“è³ªä¿è¨¼
```typescript
export function validateResponse(response: any): boolean {
  // ç©ºãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯
  if (!response.output_text || response.output_text.trim().length === 0) {
    return false;
  }

  // æ—¥æœ¬èªãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯
  const hasJapanese = /[\u3040-\u309f\u30a0-\u30ff\u4e00-\u9faf]/.test(response.output_text);

  // ITé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
  const hasRelevantContent = /ã‚·ã‚¹ãƒ†ãƒ |ã‚µãƒ¼ãƒ|ã‚¨ãƒ©ãƒ¼|å•é¡Œ|è§£æ±º|è¨­å®š|ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯/.test(response.output_text);

  return hasJapanese && hasRelevantContent;
}
```

## ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### 1. ä¸¦åˆ—å‡¦ç†
```typescript
// è¤‡æ•°ã®ã‚¯ã‚¨ãƒªã‚’ä¸¦åˆ—å®Ÿè¡Œ
const queries = [
  'ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°ã®åˆ†æ',
  'ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çŠ¶æ…‹ã®ç¢ºèª',
  'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯'
];

const results = await Promise.allSettled(
  queries.map(query => client.responses.create({
    model: 'gpt-5',
    input: query
  }))
);

const successfulResults = results
  .filter((result): result is PromiseFulfilledResult<any> => result.status === 'fulfilled')
  .map(result => result.value);
```

### 2. ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥
```typescript
export class OpenAICache {
  private cache: Map<string, { response: any; timestamp: number }> = new Map();
  private ttl: number = 3600000; // 1æ™‚é–“

  async getCachedResponse(input: string, model: string): Promise<any | null> {
    const key = `${model}:${Buffer.from(input).toString('base64')}`;
    const cached = this.cache.get(key);

    if (cached && Date.now() - cached.timestamp < this.ttl) {
      return cached.response;
    }

    return null;
  }

  setCachedResponse(input: string, model: string, response: any): void {
    const key = `${model}:${Buffer.from(input).toString('base64')}`;
    this.cache.set(key, {
      response,
      timestamp: Date.now()
    });
  }
}
```

ã“ã®ã‚¬ã‚¤ãƒ‰ã¯ã€OpenAI Node.js SDKã®åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•ã‹ã‚‰ã€TechSapoãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã®é«˜åº¦ãªçµ±åˆãƒ‘ã‚¿ãƒ¼ãƒ³ã¾ã§ã‚’åŒ…æ‹¬çš„ã«ã‚«ãƒãƒ¼ã—ã¦ã„ã¾ã™ã€‚