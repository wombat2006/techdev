/**
 * Multi-LLM Session Handler
 * CLAUDE.mdæº–æ‹ : 2å›ç›®ä»¥é™ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ç¶™ç¶šã§ç•°ãªã‚‹LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
 * ãƒ•ãƒ­ãƒ¼: User -> Claude Code -> Wall-Bounce -> GPT-5 -> Gemini-2.5-Pro -> Claude Sonnet4 -> Claude Code -> User
 */

import { WallBounceAnalyzer } from './wall-bounce-analyzer';
import { CodexMCPWrapper } from './codex-mcp-wrapper';
import { getCodexSessionManager, CodexSessionData } from './codex-session-manager';
import { logger } from '../utils/logger';

export interface LLMProviderConfig {
  name: string;
  model: string;
  priority: number;
  specialization: 'coding' | 'analysis' | 'creative' | 'general';
}

export interface MultiLLMSessionConfig {
  minProviders: number;
  maxProviders: number;
  requireConsensus: boolean;
  confidenceThreshold: number;
  rotationPolicy: 'round-robin' | 'expertise-based' | 'random';
}

export interface MultiLLMResponse {
  success: boolean;
  sessionId: string;
  conversationId: string;
  response?: string;
  error?: string;
  metadata: {
    primaryLLM: string;
    wallBounceProviders: string[];
    consensusConfidence: number;
    totalCost: number;
    processingTimeMs: number;
    turnNumber: number;
    routingStrategy: string;
  };
}

export class MultiLLMSessionHandler {
  private wallBounceAnalyzer: WallBounceAnalyzer;
  private codexWrapper: CodexMCPWrapper;
  private sessionManager: ReturnType<typeof getCodexSessionManager>;

  // CLAUDE.mdæº–æ‹ ã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æ§‹æˆ
  private llmProviders: LLMProviderConfig[] = [
    { name: 'openai-gpt5', model: 'gpt-5', priority: 1, specialization: 'coding' },
    { name: 'google-gemini', model: 'gemini-2.5-pro', priority: 2, specialization: 'analysis' },
    { name: 'anthropic-sonnet4', model: 'claude-sonnet-4', priority: 3, specialization: 'creative' },
    { name: 'openai-codex', model: 'gpt-5-codex', priority: 4, specialization: 'coding' }
  ];

  constructor() {
    this.wallBounceAnalyzer = new WallBounceAnalyzer();
    this.codexWrapper = new CodexMCPWrapper();
    this.sessionManager = getCodexSessionManager();
  }

  /**
   * ã‚»ãƒƒã‚·ãƒ§ãƒ³ç¶™ç¶šæ™‚ã®ãƒãƒ«ãƒLLMãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°å‡¦ç†
   */
  async continueSessionWithWallBounce(args: {
    sessionId: string;
    prompt: string;
    userId?: string;
  }): Promise<MultiLLMResponse> {
    const startTime = Date.now();

    try {
      // æ—¢å­˜ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—
      const session = await this.sessionManager.getSession(args.sessionId);
      if (!session) {
        return this.createErrorResponse(args.sessionId, '', 'Session not found', startTime);
      }

      const turnNumber = session.messages.length || 1;
      logger.info(`ğŸ”„ Multi-LLM routing for turn ${turnNumber}`, {
        sessionId: args.sessionId,
        conversationId: session.conversationId,
        turnNumber
      });

      // ã‚¿ãƒ¼ãƒ³1: ç›´æ¥Codexå®Ÿè¡Œ
      if (turnNumber === 1) {
        return await this.executeDirectCodex(args, session, startTime);
      }

      // ã‚¿ãƒ¼ãƒ³2ä»¥é™: Wall-Bounceåˆ†æå®Ÿè¡Œ
      return await this.executeWallBounceAnalysis(args, session, turnNumber, startTime);

    } catch (error) {
      logger.error('MultiLLMSessionHandler error:', error);
      return this.createErrorResponse(
        args.sessionId,
        '',
        error instanceof Error ? error.message : 'Unknown error',
        startTime
      );
    }
  }

  /**
   * ã‚¿ãƒ¼ãƒ³1: ç›´æ¥Codexå®Ÿè¡Œ
   */
  private async executeDirectCodex(
    args: { sessionId: string; prompt: string; userId?: string },
    session: CodexSessionData,
    startTime: number
  ): Promise<MultiLLMResponse> {
    logger.info('ğŸ¯ Turn 1: Direct Codex execution');

    const result = await this.codexWrapper.continueCodex({
      sessionId: args.sessionId,
      prompt: args.prompt,
      userId: args.userId
    });

    return {
      success: result.success,
      sessionId: result.sessionId,
      conversationId: result.conversationId,
      response: result.response,
      error: result.error,
      metadata: {
        primaryLLM: 'openai-codex',
        wallBounceProviders: [],
        consensusConfidence: 1.0,
        totalCost: 0.01, // Codexæ¨å®šã‚³ã‚¹ãƒˆ
        processingTimeMs: Date.now() - startTime,
        turnNumber: 1,
        routingStrategy: 'direct-codex'
      }
    };
  }

  /**
   * ã‚¿ãƒ¼ãƒ³2ä»¥é™: Wall-Bounceåˆ†æå®Ÿè¡Œ
   */
  private async executeWallBounceAnalysis(
    args: { sessionId: string; prompt: string; userId?: string },
    session: CodexSessionData,
    turnNumber: number,
    startTime: number
  ): Promise<MultiLLMResponse> {
    logger.info(`ğŸ“ Turn ${turnNumber}: Wall-Bounce multi-LLM analysis`);

    // ä¼šè©±å±¥æ­´ã‚’å–å¾—ã—ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
    const conversationHistory = await this.sessionManager.getConversationHistory(args.sessionId);
    const contextPrompt = this.buildContextualPrompt(args.prompt, conversationHistory, turnNumber);

    // Wall-Bounceè¨­å®šã‚’ã‚¿ãƒ¼ãƒ³æ•°ã«å¿œã˜ã¦èª¿æ•´
    const config = this.getWallBounceConfig(turnNumber);
    logger.debug('Wall-bounce configuration applied', {
      turnNumber,
      config
    });

    // Wall-Bounceåˆ†æå®Ÿè¡Œ
    const wallBounceResult = await this.wallBounceAnalyzer.executeWallBounce(
      contextPrompt,
      { taskType: 'premium' }
    );

    // ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’æ›´æ–°
    await this.sessionManager.addAssistantResponse(args.sessionId, wallBounceResult.consensus.content);

    return {
      success: true,
      sessionId: args.sessionId,
      conversationId: session.conversationId,
      response: wallBounceResult.consensus.content,
      metadata: {
        primaryLLM: wallBounceResult.llm_votes[0]?.provider || 'unknown',
        wallBounceProviders: wallBounceResult.debug.providers_used,
        consensusConfidence: wallBounceResult.consensus.confidence,
        totalCost: wallBounceResult.total_cost,
        processingTimeMs: Date.now() - startTime,
        turnNumber,
        routingStrategy: 'wall-bounce-consensus'
      }
    };
  }

  /**
   * æ–‡è„ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
   */
  private buildContextualPrompt(
    currentPrompt: string,
    conversationHistory: any[],
    turnNumber: number
  ): string {
    const lastMessages = conversationHistory.slice(-4); // ç›´è¿‘4ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    const context = lastMessages
      .map(msg => `${msg.type}: ${msg.content}`)
      .join('\n\n');

    return `
## ç¶™ç¶šã‚»ãƒƒã‚·ãƒ§ãƒ³ (Turn ${turnNumber})

### ä¼šè©±å±¥æ­´:
${context}

### ç¾åœ¨ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ:
${currentPrompt}

### åˆ†æè¦æ±‚:
CLAUDE.mdå£æ‰“ã¡åŸå‰‡ã«å¾“ã„ã€ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:
1. å‰å›ã®å›ç­”ã‚’ç†è§£ã—ã€æ–‡è„ˆã‚’ç¶™ç¶š
2. æ–°ã—ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«å¯¾ã™ã‚‹æœ€é©ãªå›ç­”ã‚’ç”Ÿæˆ
3. ã‚³ãƒ¼ãƒ‰æ”¹è‰¯ãƒ»æ‹¡å¼µã®å ´åˆã¯å…·ä½“çš„ãªå®Ÿè£…ã‚’æç¤º
4. æŠ€è¡“çš„æ ¹æ‹ ã¨å®Ÿç”¨æ€§ã‚’é‡è¦–ã—ãŸå›ç­”
`.trim();
  }

  /**
   * ã‚¿ãƒ¼ãƒ³æ•°ã«å¿œã˜ãŸWall-Bounceè¨­å®š
   */
  private getWallBounceConfig(turnNumber: number): MultiLLMSessionConfig {
    if (turnNumber === 2) {
      // ã‚¿ãƒ¼ãƒ³2: GPT-5 + Gemini-2.5-Pro
      return {
        minProviders: 2,
        maxProviders: 2,
        requireConsensus: true,
        confidenceThreshold: 0.8,
        rotationPolicy: 'expertise-based'
      };
    } else if (turnNumber === 3) {
      // ã‚¿ãƒ¼ãƒ³3: GPT-5 + Gemini-2.5-Pro + Claude Sonnet4
      return {
        minProviders: 3,
        maxProviders: 3,
        requireConsensus: true,
        confidenceThreshold: 0.85,
        rotationPolicy: 'expertise-based'
      };
    } else {
      // ã‚¿ãƒ¼ãƒ³4ä»¥é™: å…¨LLMæŠ•ç¥¨
      return {
        minProviders: 3,
        maxProviders: 4,
        requireConsensus: true,
        confidenceThreshold: 0.9,
        rotationPolicy: 'round-robin'
      };
    }
  }

  /**
   * ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ
   */
  private createErrorResponse(
    sessionId: string,
    conversationId: string,
    error: string,
    startTime: number
  ): MultiLLMResponse {
    return {
      success: false,
      sessionId,
      conversationId,
      error,
      metadata: {
        primaryLLM: 'none',
        wallBounceProviders: [],
        consensusConfidence: 0,
        totalCost: 0,
        processingTimeMs: Date.now() - startTime,
        turnNumber: 0,
        routingStrategy: 'error'
      }
    };
  }

  /**
   * ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆå–å¾—
   */
  async getSessionStats(sessionId: string): Promise<{
    totalTurns: number;
    llmProviderUsage: Record<string, number>;
    totalCost: number;
    averageConfidence: number;
  }> {
    const conversationHistory = await this.sessionManager.getConversationHistory(sessionId);

    // çµ±è¨ˆè¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…
    return {
      totalTurns: Math.floor(conversationHistory.length / 2),
      llmProviderUsage: {
        'openai-codex': 1,
        'wall-bounce-consensus': Math.floor(conversationHistory.length / 2) - 1
      },
      totalCost: conversationHistory.length * 0.05, // æ¨å®š
      averageConfidence: 0.85 // æ¨å®š
    };
  }
}

export const getMultiLLMSessionHandler = (): MultiLLMSessionHandler => {
  return new MultiLLMSessionHandler();
};
